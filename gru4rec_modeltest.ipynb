{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\KNOWLEDGES\\Computer_Science\\ANACONDA\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "E:\\KNOWLEDGES\\Computer_Science\\ANACONDA\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "E:\\KNOWLEDGES\\Computer_Science\\ANACONDA\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "E:\\KNOWLEDGES\\Computer_Science\\ANACONDA\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "E:\\KNOWLEDGES\\Computer_Science\\ANACONDA\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "E:\\KNOWLEDGES\\Computer_Science\\ANACONDA\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "E:\\KNOWLEDGES\\Computer_Science\\ANACONDA\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "E:\\KNOWLEDGES\\Computer_Science\\ANACONDA\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "E:\\KNOWLEDGES\\Computer_Science\\ANACONDA\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "E:\\KNOWLEDGES\\Computer_Science\\ANACONDA\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "E:\\KNOWLEDGES\\Computer_Science\\ANACONDA\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "E:\\KNOWLEDGES\\Computer_Science\\ANACONDA\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set parameters\n",
    "\n",
    "ROOT = 'C:/Users/BuiQuocBao/OneDrive/Desktop/GRU4REC/processed/'\n",
    "DATA_TYPE = 'sample'\n",
    "PATH_TO_TRAIN = ROOT + 'rsc15_train_{}.txt'.format(DATA_TYPE)\n",
    "PATH_TO_TEST = ROOT + 'rsc15_test_{}.txt'.format(DATA_TYPE)\n",
    "checkpoint_dir = './checkpoint'\n",
    "if not os.path.exists(checkpoint_dir): os.mkdir(checkpoint_dir)\n",
    "        \n",
    "layers = 1\n",
    "rnn_size = 100\n",
    "batch_size = 50\n",
    "drop_keep_prob = 0.7\n",
    "\n",
    "n_epochs = 3\n",
    "learning_rate = 0.001\n",
    "decay = 0.96\n",
    "decay_steps = 1e4\n",
    "grad_cap = 0\n",
    "print_step = 1e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data\n",
    "data = pd.read_csv(PATH_TO_TRAIN, sep='\\t', dtype={'ItemId': np.int64})\n",
    "valid = pd.read_csv(PATH_TO_TEST, sep='\\t', dtype={'ItemId': np.int64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.07 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SessionId</th>\n",
       "      <th>ItemId</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>ItemIdx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>214701242</td>\n",
       "      <td>1.396804e+09</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>214826623</td>\n",
       "      <td>1.396804e+09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "      <td>214838503</td>\n",
       "      <td>1.396861e+09</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21</td>\n",
       "      <td>214838503</td>\n",
       "      <td>1.396861e+09</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>214838503</td>\n",
       "      <td>1.396861e+09</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21</td>\n",
       "      <td>214838503</td>\n",
       "      <td>1.396862e+09</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>214838503</td>\n",
       "      <td>1.396862e+09</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>21</td>\n",
       "      <td>214548744</td>\n",
       "      <td>1.396862e+09</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>36</td>\n",
       "      <td>214551594</td>\n",
       "      <td>1.396814e+09</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>36</td>\n",
       "      <td>214586970</td>\n",
       "      <td>1.396814e+09</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SessionId     ItemId     timestamp  ItemIdx\n",
       "0          6  214701242  1.396804e+09        0\n",
       "1          6  214826623  1.396804e+09        1\n",
       "2         21  214838503  1.396861e+09        2\n",
       "3         21  214838503  1.396861e+09        2\n",
       "4         21  214838503  1.396861e+09        2\n",
       "5         21  214838503  1.396862e+09        2\n",
       "6         21  214838503  1.396862e+09        2\n",
       "7         21  214548744  1.396862e+09        3\n",
       "8         36  214551594  1.396814e+09        4\n",
       "9         36  214586970  1.396814e+09        5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## add item index \n",
    "##\n",
    "itemids = data['ItemId'].unique()\n",
    "n_items = len(itemids)\n",
    "itemidmap = pd.Series(data=np.arange(n_items), index=itemids).to_dict()\n",
    "%time data['ItemIdx'] = data['ItemId'].map(lambda x: itemidmap[x])\n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  2,  8, 10, 15, 19, 24, 27, 32, 34])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## offset sessions\n",
    "## EX : SessionId 6 : start 0\n",
    "##      SessionId 21 : start 2\n",
    "##      SessionId 36 : start 8\n",
    "offset_sessions = np.zeros(data['SessionId'].nunique()+1, dtype=np.int32)\n",
    "offset_sessions[1:] = data.groupby('SessionId').size().cumsum()\n",
    "offset_sessions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PREPARE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## placeholder & learning rate\n",
    "## tf.placeholder() : Allocated storage for data (such as for image pixel data during a feed)\n",
    "X = tf.placeholder(tf.int32, [batch_size], name='input')\n",
    "Y = tf.placeholder(tf.int32, [batch_size], name='output')\n",
    "## for _ in range() : Ignore the index\n",
    "States = [tf.placeholder(tf.float32, [batch_size, rnn_size], name='rnn_state') for _ in range(layers)]\n",
    "## tf.Variable you have to provide an initial value when you declare it. \n",
    "## tf.placeholder you don't have to provide an initial value \n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "## When training a model, it is often recommended to lower the learning rate as the training progresses. \n",
    "## This function applies an exponential decay function to a provided initial learning rate. \n",
    "## It requires a global_step value to compute the decayed learning rate. \n",
    "## You can just pass a TensorFlow variable that you increment at each training step.\n",
    "lr = tf.maximum(1e-5,tf.train.exponential_decay(learning_rate, global_step, decay_steps, decay, staircase=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\KNOWLEDGES\\Computer_Science\\ANACONDA\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "## gru weigths\n",
    "##\n",
    "with tf.variable_scope('gru_layer', reuse=tf.AUTO_REUSE):\n",
    "    #sigma = sigma if sigma != 0 else np.sqrt(6.0 / (n_items + rnn_size))\n",
    "    #initializer = tf.random_uniform_initializer(minval=-sigma, maxval=sigma)\n",
    "    initializer = tf.glorot_uniform_initializer()\n",
    "    # get_variable(name, shape, initializer) : Khoi tao bien\n",
    "    # initializer : Initializer for the variable if one is created. Can either be an initializer object or a Tensor. \n",
    "    #               If it's a Tensor, its shape must be known unless validate_shape is False.\n",
    "    embedding = tf.get_variable('embedding', [n_items, rnn_size], initializer=initializer)\n",
    "    softmax_W = tf.get_variable('softmax_w', [n_items, rnn_size], initializer=initializer)\n",
    "    softmax_b = tf.get_variable('softmax_b', [n_items], initializer=tf.zeros_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-a24258f2adb7>:5: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-10-a24258f2adb7>:13: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    }
   ],
   "source": [
    "## gru_cell\n",
    "##\n",
    "with tf.variable_scope('gru_cell', reuse=tf.AUTO_REUSE):\n",
    "    # rnn_size : int, The number of units in the GRU cell.\n",
    "    cell = tf.nn.rnn_cell.GRUCell(rnn_size, activation=tf.nn.tanh)\n",
    "    # DropoutWrapper(cell, output_keep_prob)\n",
    "    # cell : an RNNCell, a projection to output_size is added to it.\n",
    "    # output_keep_prob : unit Tensor or float between 0 and 1, output keep probability; \n",
    "    #                    if it is constant and 1, no output dropout will be added.\n",
    "    drop_cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=drop_keep_prob)\n",
    "    # MultiRNNCell(cells, state_is_tuple=True) : RNN cell composed sequentially of multiple simple cells.\n",
    "    # cells : list of RNNCells that will be composed in this order.\n",
    "    stacked_cell = tf.nn.rnn_cell.MultiRNNCell([drop_cell] * layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x000001C5A485FA88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x000001C5A485FA88>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x000001C5A485FA88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x000001C5A485FA88>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:From E:\\KNOWLEDGES\\Computer_Science\\ANACONDA\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:564: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From E:\\KNOWLEDGES\\Computer_Science\\ANACONDA\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:574: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Entity <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x000001C5A469EE48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x000001C5A469EE48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x000001C5A469EE48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method GRUCell.call of <tensorflow.python.ops.rnn_cell_impl.GRUCell object at 0x000001C5A469EE48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "## feedforward gur_cell\n",
    "## EX : seesion 1 : item sequence 5, 7, 9\n",
    "## outputs of 7 is final_state\n",
    "\n",
    "inputs = tf.nn.embedding_lookup(embedding, X)\n",
    "output, state_ = stacked_cell(inputs, tuple(States))\n",
    "final_state = state_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for training\n",
    "sampled_W = tf.nn.embedding_lookup(softmax_W, Y)\n",
    "sampled_b = tf.nn.embedding_lookup(softmax_b, Y)\n",
    "logits = tf.matmul(output, sampled_W, transpose_b=True) + sampled_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cross-entropy loss\n",
    "yhat = tf.nn.softmax(logits)\n",
    "cost = tf.reduce_mean(-tf.log(tf.diag_part(yhat)+1e-24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for prediction\n",
    "logits_all = tf.matmul(output, softmax_W, transpose_b=True) + softmax_b\n",
    "yhat_all = tf.nn.softmax(logits_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## optimize\n",
    "## Adam optimizer\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "tvars = tf.trainable_variables()\n",
    "gvs = optimizer.compute_gradients(cost, tvars)\n",
    "if grad_cap > 0:\n",
    "    capped_gvs = [(tf.clip_by_norm(grad, grad_cap), var) for grad, var in gvs]\n",
    "else:\n",
    "    capped_gvs = gvs \n",
    "train_op = optimizer.apply_gradients(capped_gvs, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## session start\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(tf.global_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UNDERSTANDING DATA FEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4] ... [45 46 47 48 49]\n",
      "49\n",
      "[ 0  2  8 10 15 19 24 27 32 34]\n",
      "[ 2  8 10 15 19 24 27 32 34 36]\n"
     ]
    }
   ],
   "source": [
    "## data row index of start of session id\n",
    "iters = np.arange(batch_size)\n",
    "maxiter = iters.max()\n",
    "print(iters[:5], \"...\", iters[-5:])\n",
    "print(maxiter)\n",
    "start = offset_sessions[iters]\n",
    "end = offset_sessions[iters+1]\n",
    "print(start[:10])\n",
    "print(end[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    SessionId     ItemId     timestamp  ItemIdx\n",
      "0           6  214701242  1.396804e+09        0\n",
      "1           6  214826623  1.396804e+09        1\n",
      "2          21  214838503  1.396861e+09        2\n",
      "3          21  214838503  1.396861e+09        2\n",
      "4          21  214838503  1.396861e+09        2\n",
      "5          21  214838503  1.396862e+09        2\n",
      "6          21  214838503  1.396862e+09        2\n",
      "7          21  214548744  1.396862e+09        3\n",
      "8          36  214551594  1.396814e+09        4\n",
      "9          36  214586970  1.396814e+09        5\n",
      "10         41  214821277  1.396773e+09        6\n",
      "11         41  214821277  1.396773e+09        6\n",
      "12         41  214821277  1.396773e+09        6\n",
      "13         41  214821277  1.396773e+09        6\n",
      "14         41  214821277  1.396773e+09        6\n",
      "15         53  214544355  1.396449e+09        7\n",
      "16         53  214601212  1.396449e+09        8\n",
      "17         53  214601212  1.396449e+09        8\n",
      "18         53  214601212  1.396449e+09        8\n",
      "19         56  214832557  1.396543e+09        9\n",
      "2\n",
      "[ 0  2  4  6  7  9 13 16 18 20]\n"
     ]
    }
   ],
   "source": [
    "## start item index of each session\n",
    "minlen = (end-start).min()\n",
    "print(data[:20])\n",
    "out_idx = data.ItemIdx.values[start]\n",
    "print(minlen)\n",
    "print(out_idx[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  2  4  6  7  9 13 16 18 20]\n",
      "[ 1  2  5  6  8 10 14 16 19 21]\n"
     ]
    }
   ],
   "source": [
    "## in_idx : start item index of each session\n",
    "## out_idx : end item index of each session\n",
    "i = 0\n",
    "in_idx = out_idx\n",
    "out_idx = data.ItemIdx.values[start+i+1]\n",
    "print(in_idx[:10])\n",
    "print(out_idx[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2  8 10 15 19 24 27 32 34 36]\n",
      "[ 1  3  9 11 16 20 25 28 33 35]\n",
      "[ 0  2  8  9 10 14 17 25 27 28]\n"
     ]
    }
   ],
   "source": [
    "start = start+minlen-1\n",
    "\n",
    "mask = np.arange(len(iters))[(end-start)<=1]\n",
    "print(end[:10])\n",
    "print(start[:10])\n",
    "print(mask[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "print(len(mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50  1 51  3  4  5  6  7  8  9]\n",
      "[186   3 188  11  16  20  25  28  33  35]\n",
      "[188   8 191  15  19  24  27  32  34  36]\n"
     ]
    }
   ],
   "source": [
    "iters[0] = 50\n",
    "iters[2] = 51\n",
    "print(iters[:10])\n",
    "###\n",
    "start[0] = offset_sessions[50]\n",
    "end[0] = offset_sessions[50+1]\n",
    "start[2] = offset_sessions[51]\n",
    "end[2] = offset_sessions[51+1]\n",
    "print(start[:10])\n",
    "print(end[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[186   3 188  11  16  20  25  28 191 195]\n",
      "[188   8 191  15  19  24  27  32 195 198]\n"
     ]
    }
   ],
   "source": [
    "for idx in mask:\n",
    "    maxiter += 1\n",
    "    if maxiter >= len(offset_sessions)-1:\n",
    "        finished = True\n",
    "        break\n",
    "    iters[idx] = maxiter\n",
    "    start[idx] = offset_sessions[maxiter]\n",
    "    end[idx] = offset_sessions[maxiter+1]\n",
    "print(start[:10])\n",
    "print(end[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3140308\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SessionId</th>\n",
       "      <th>ItemId</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>ItemIdx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3140305</th>\n",
       "      <td>11562131</td>\n",
       "      <td>214854542</td>\n",
       "      <td>1.411823e+09</td>\n",
       "      <td>21584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3140306</th>\n",
       "      <td>11562151</td>\n",
       "      <td>214536296</td>\n",
       "      <td>1.411769e+09</td>\n",
       "      <td>3483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3140307</th>\n",
       "      <td>11562151</td>\n",
       "      <td>214536296</td>\n",
       "      <td>1.411769e+09</td>\n",
       "      <td>3483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3140308</th>\n",
       "      <td>11562157</td>\n",
       "      <td>214580372</td>\n",
       "      <td>1.411648e+09</td>\n",
       "      <td>1881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3140309</th>\n",
       "      <td>11562157</td>\n",
       "      <td>214516012</td>\n",
       "      <td>1.411648e+09</td>\n",
       "      <td>1880</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         SessionId     ItemId     timestamp  ItemIdx\n",
       "3140305   11562131  214854542  1.411823e+09    21584\n",
       "3140306   11562151  214536296  1.411769e+09     3483\n",
       "3140307   11562151  214536296  1.411769e+09     3483\n",
       "3140308   11562157  214580372  1.411648e+09     1881\n",
       "3140309   11562157  214516012  1.411648e+09     1880"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finished = False\n",
    "endpoint_count = 0\n",
    "while not finished:\n",
    "    minlen = (end-start).min()\n",
    "    out_idx = data.ItemIdx.values[start]\n",
    "    for i in range(minlen-1):\n",
    "        in_idx = out_idx\n",
    "        out_idx = data.ItemIdx.values[start+i+1]\n",
    "        endpoint_count += len(out_idx)\n",
    "    \n",
    "    start = start+minlen-1\n",
    "    mask = np.arange(len(iters))[(end-start)<=1]\n",
    "\n",
    "    for idx in mask:\n",
    "        maxiter += 1\n",
    "        if maxiter >= len(offset_sessions)-1:\n",
    "            finished = True\n",
    "            break\n",
    "        iters[idx] = maxiter\n",
    "        start[idx] = offset_sessions[maxiter]\n",
    "        end[idx] = offset_sessions[maxiter+1]\n",
    "        \n",
    "print(max(start))\n",
    "data[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2348300\n",
      "2348603\n"
     ]
    }
   ],
   "source": [
    "print(endpoint_count)\n",
    "print(sum(data.groupby('SessionId').size() - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\tStep 1\tlr: 0.00100\tloss: 3.9120\tElapsed: 0.2\n",
      "Epoch 0\tStep 1000\tlr: 0.00100\tloss: 3.3758\tElapsed: 44.8\n",
      "Epoch 0\tStep 2000\tlr: 0.00100\tloss: 3.0282\tElapsed: 96.7\n",
      "Epoch 0\tStep 3000\tlr: 0.00100\tloss: 2.8469\tElapsed: 146.9\n",
      "Epoch 0\tStep 4000\tlr: 0.00100\tloss: 2.6973\tElapsed: 240.8\n",
      "Epoch 0\tStep 5000\tlr: 0.00100\tloss: 2.6036\tElapsed: 336.8\n",
      "Epoch 0\tStep 6000\tlr: 0.00100\tloss: 2.5199\tElapsed: 485.5\n",
      "Epoch 0\tStep 7000\tlr: 0.00100\tloss: 2.4696\tElapsed: 736.9\n",
      "Epoch 0\tStep 8000\tlr: 0.00100\tloss: 2.4117\tElapsed: 994.3\n",
      "Epoch 0\tStep 9000\tlr: 0.00100\tloss: 2.3730\tElapsed: 1246.8\n",
      "Epoch 0\tStep 10000\tlr: 0.00100\tloss: 2.3305\tElapsed: 1533.0\n",
      "Epoch 0\tStep 11000\tlr: 0.00096\tloss: 2.3054\tElapsed: 1813.5\n",
      "Epoch 0\tStep 12000\tlr: 0.00096\tloss: 2.2729\tElapsed: 2094.3\n",
      "Epoch 0\tStep 13000\tlr: 0.00096\tloss: 2.2506\tElapsed: 2375.4\n",
      "Epoch 0\tStep 14000\tlr: 0.00096\tloss: 2.2250\tElapsed: 2654.7\n",
      "Epoch 0\tStep 15000\tlr: 0.00096\tloss: 2.2004\tElapsed: 2942.8\n",
      "Epoch 0\tStep 16000\tlr: 0.00096\tloss: 2.1742\tElapsed: 3137.4\n",
      "Epoch 0\tStep 17000\tlr: 0.00096\tloss: 2.1585\tElapsed: 3238.4\n",
      "Epoch 0\tStep 18000\tlr: 0.00096\tloss: 2.1404\tElapsed: 3336.7\n",
      "Epoch 0\tStep 19000\tlr: 0.00096\tloss: 2.1229\tElapsed: 3433.7\n",
      "Epoch 0\tStep 20000\tlr: 0.00096\tloss: 2.1110\tElapsed: 3528.0\n",
      "Epoch 0\tStep 21000\tlr: 0.00092\tloss: 2.0978\tElapsed: 3622.5\n",
      "Epoch 0\tStep 22000\tlr: 0.00092\tloss: 2.0874\tElapsed: 3722.3\n",
      "Epoch 0\tStep 23000\tlr: 0.00092\tloss: 2.0765\tElapsed: 3825.2\n",
      "Epoch 0\tStep 24000\tlr: 0.00092\tloss: 2.0677\tElapsed: 3950.1\n",
      "Epoch 0\tStep 25000\tlr: 0.00092\tloss: 2.0621\tElapsed: 4051.8\n",
      "Epoch 0\tStep 26000\tlr: 0.00092\tloss: 2.0559\tElapsed: 4162.8\n",
      "Epoch 0\tStep 27000\tlr: 0.00092\tloss: 2.0467\tElapsed: 4298.4\n",
      "Epoch 0\tStep 28000\tlr: 0.00092\tloss: 2.0385\tElapsed: 4418.6\n",
      "Epoch 0\tStep 29000\tlr: 0.00092\tloss: 2.0310\tElapsed: 4524.3\n",
      "Epoch 0\tStep 30000\tlr: 0.00092\tloss: 2.0212\tElapsed: 4641.4\n",
      "Epoch 0\tStep 31000\tlr: 0.00088\tloss: 2.0131\tElapsed: 4756.6\n",
      "Epoch 0\tStep 32000\tlr: 0.00088\tloss: 2.0057\tElapsed: 4887.5\n",
      "Epoch 0\tStep 33000\tlr: 0.00088\tloss: 1.9986\tElapsed: 5050.8\n",
      "Epoch 0\tStep 34000\tlr: 0.00088\tloss: 1.9931\tElapsed: 5254.4\n",
      "Epoch 0\tStep 35000\tlr: 0.00088\tloss: 1.9876\tElapsed: 5413.1\n",
      "Epoch 0\tStep 36000\tlr: 0.00088\tloss: 1.9814\tElapsed: 5615.2\n",
      "Epoch 0\tStep 37000\tlr: 0.00088\tloss: 1.9766\tElapsed: 5832.0\n",
      "Epoch 0\tStep 38000\tlr: 0.00088\tloss: 1.9698\tElapsed: 5987.0\n",
      "Epoch 0\tStep 39000\tlr: 0.00088\tloss: 1.9664\tElapsed: 6110.0\n",
      "Epoch 0\tStep 40000\tlr: 0.00088\tloss: 1.9607\tElapsed: 6213.9\n",
      "Epoch 0\tStep 41000\tlr: 0.00085\tloss: 1.9600\tElapsed: 6315.4\n",
      "Epoch 0\tStep 42000\tlr: 0.00085\tloss: 1.9575\tElapsed: 6411.0\n",
      "Epoch 0\tStep 43000\tlr: 0.00085\tloss: 1.9566\tElapsed: 6504.8\n",
      "Epoch 0\tStep 44000\tlr: 0.00085\tloss: 1.9540\tElapsed: 6598.9\n",
      "Epoch 0\tStep 45000\tlr: 0.00085\tloss: 1.9502\tElapsed: 6698.8\n",
      "Epoch 0\tStep 46000\tlr: 0.00085\tloss: 1.9445\tElapsed: 6828.9\n",
      "Epoch 1\tStep 47000\tlr: 0.00085\tloss: 1.9292\tElapsed: 6947.6\n",
      "Epoch 1\tStep 48000\tlr: 0.00085\tloss: 1.7005\tElapsed: 7044.1\n",
      "Epoch 1\tStep 49000\tlr: 0.00085\tloss: 1.6346\tElapsed: 7137.8\n",
      "Epoch 1\tStep 50000\tlr: 0.00085\tloss: 1.5985\tElapsed: 7237.2\n",
      "Epoch 1\tStep 51000\tlr: 0.00082\tloss: 1.5618\tElapsed: 7366.0\n",
      "Epoch 1\tStep 52000\tlr: 0.00082\tloss: 1.5335\tElapsed: 7525.2\n",
      "Epoch 1\tStep 53000\tlr: 0.00082\tloss: 1.5093\tElapsed: 7740.8\n",
      "Epoch 1\tStep 54000\tlr: 0.00082\tloss: 1.5181\tElapsed: 7982.1\n",
      "Epoch 1\tStep 55000\tlr: 0.00082\tloss: 1.5236\tElapsed: 8143.4\n",
      "Epoch 1\tStep 56000\tlr: 0.00082\tloss: 1.5297\tElapsed: 8281.7\n",
      "Epoch 1\tStep 57000\tlr: 0.00082\tloss: 1.5287\tElapsed: 8396.9\n",
      "Epoch 1\tStep 58000\tlr: 0.00082\tloss: 1.5436\tElapsed: 8499.7\n",
      "Epoch 1\tStep 59000\tlr: 0.00082\tloss: 1.5476\tElapsed: 8616.7\n",
      "Epoch 1\tStep 60000\tlr: 0.00082\tloss: 1.5533\tElapsed: 8815.7\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_cost = []\n",
    "    state = [np.zeros([batch_size, rnn_size], dtype=np.float32) for _ in range(layers)]\n",
    "    iters = np.arange(batch_size)\n",
    "    maxiter = iters.max()\n",
    "    \n",
    "    start = offset_sessions[iters]\n",
    "    end = offset_sessions[iters+1]\n",
    "    \n",
    "    finished = False\n",
    "    while not finished:\n",
    "        minlen = (end-start).min()\n",
    "        out_idx = data.ItemIdx.values[start]\n",
    "        for i in range(minlen-1):\n",
    "            in_idx = out_idx\n",
    "            out_idx = data.ItemIdx.values[start+i+1]\n",
    "            # prepare inputs, targeted outputs and hidden states\n",
    "            fetches = [cost, final_state, global_step, lr, train_op]\n",
    "            feed_dict = {X: in_idx, Y: out_idx}\n",
    "            for j in range(layers): \n",
    "                feed_dict[States[j]] = state[j]\n",
    "            \n",
    "            cost_, state, step, lr_, _ = sess.run(fetches, feed_dict)\n",
    "            epoch_cost.append(cost_)\n",
    "            \n",
    "            if step == 1 or step % print_step == 0:\n",
    "                avgc = np.mean(epoch_cost)\n",
    "                print('Epoch {}\\tStep {}\\tlr: {:.5f}\\tloss: {:.4f}\\tElapsed: {:.1f}'.\n",
    "                      format(epoch, step, lr_, avgc, time.time()-tic))\n",
    "\n",
    "        start = start+minlen-1\n",
    "        mask = np.arange(len(iters))[(end-start)<=1]\n",
    "        for idx in mask:\n",
    "            maxiter += 1\n",
    "            if maxiter >= len(offset_sessions)-1:\n",
    "                finished = True\n",
    "                break\n",
    "            iters[idx] = maxiter\n",
    "            start[idx] = offset_sessions[maxiter]\n",
    "            end[idx] = offset_sessions[maxiter+1]\n",
    "        if len(mask):\n",
    "            for i in range(layers):\n",
    "                state[i][mask] = 0\n",
    "        \n",
    "    avgc = np.mean(epoch_cost)\n",
    "    if np.isnan(avgc):\n",
    "        print('Epoch {}: Nan error!'.format(epoch, avgc))\n",
    "        break\n",
    "    saver.save(sess, '{}/gru-model'.format(checkpoint_dir), global_step=epoch)\n",
    "print(\"1 epoch elapsed time:\", time.time() - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## parameters\n",
    "cut_off = 20     # @20\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## session restore\n",
    "##\n",
    "sess = tf.Session()\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "ckpt = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "saver.restore(sess, ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## valdation data set\n",
    "valid['ItemIdx'] = valid['ItemId'].map(lambda x: itemidmap[x])\n",
    "valid[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_sessions = np.zeros(valid['SessionId'].nunique()+1, dtype=np.int32)\n",
    "offset_sessions[1:] = valid.groupby('SessionId').size().cumsum()\n",
    "offset_sessions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## init prediction\n",
    "##\n",
    "if len(offset_sessions) - 1 < batch_size:\n",
    "    batch_size = len(offset_sessions) - 1\n",
    "### training step\n",
    "iters = np.arange(batch_size).astype(np.int32)\n",
    "maxiter = iters.max()\n",
    "start = offset_sessions[iters]\n",
    "end = offset_sessions[iters+1]\n",
    "in_idx = np.zeros(batch_size, dtype=np.int32)\n",
    "predict_state = [np.zeros([batch_size, rnn_size], dtype=np.float32) for _ in range(layers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prediction & evaluation\n",
    "## data feeding \n",
    "evalutation_point_count = 0\n",
    "mrr, recall = 0.0, 0.0\n",
    "tic = time.time()\n",
    "while True:\n",
    "    valid_mask = iters >= 0\n",
    "    if valid_mask.sum() == 0:\n",
    "        print(\"break at endpoint\", evalutation_point_count)\n",
    "        break\n",
    "        \n",
    "    start_valid = start[valid_mask]\n",
    "    minlen = (end[valid_mask]-start_valid).min()\n",
    "    in_idx[valid_mask] = valid.ItemIdx.values[start_valid]\n",
    "    \n",
    "    for i in range(minlen-1):\n",
    "        out_idx = valid.ItemIdx.values[start_valid+i+1]\n",
    "        ## --- prediction --- ##\n",
    "        fetches = [yhat_all, final_state]\n",
    "        feed_dict = {X: in_idx}\n",
    "        for j in range(layers): \n",
    "            feed_dict[States[j]] = predict_state[j]\n",
    "        preds, predict_state = sess.run(fetches, feed_dict)\n",
    "        preds = pd.DataFrame(data=np.asarray(preds).T)\n",
    "        preds.fillna(0, inplace=True) ### preds shape: (item_size, batch_size)\n",
    "        \n",
    "        ## --- evaluation --- ##\n",
    "        in_idx[valid_mask] = out_idx\n",
    "        \n",
    "        ranks = (preds.values.T[valid_mask].T > \n",
    "                 np.diag(preds.loc[in_idx].values)[valid_mask]).sum(axis=0) + 1\n",
    "        \n",
    "        rank_ok = ranks < cut_off\n",
    "        recall += rank_ok.sum()\n",
    "        mrr += (1.0 / ranks[rank_ok]).sum()\n",
    "        evalutation_point_count += len(ranks)\n",
    "        \n",
    "    start = start+minlen-1\n",
    "    mask = np.arange(len(iters))[(valid_mask) & (end-start<=1)]\n",
    "    \n",
    "    for idx in mask:\n",
    "        maxiter += 1\n",
    "        ## \n",
    "        if maxiter >= len(offset_sessions)-1:\n",
    "            iters[idx] = -1\n",
    "        else:\n",
    "            iters[idx] = maxiter\n",
    "            start[idx] = offset_sessions[maxiter]\n",
    "            end[idx] = offset_sessions[maxiter+1]\n",
    "            \n",
    "    if len(mask):\n",
    "        for i in range(layers):\n",
    "            predict_state[i][mask] = 0\n",
    "\n",
    "### \n",
    "recall = recall/evalutation_point_count\n",
    "mrr = mrr/evalutation_point_count\n",
    "print(\"recall: \", recall, \"mrr:\", mrr, \"elapsed time:\", time.time()-tic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
